<!DOCTYPE html>
<!-- saved from url=(0049)https://raid.wiki.kernel.org/index.php/RAID_setup -->
<html lang="en" dir="ltr" class="client-js"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>RAID setup - Linux Raid Wiki</title>

<meta name="generator" content="MediaWiki 1.19.24">
<link rel="shortcut icon" href="https://raid.wiki.kernel.org/favicon.ico">
<link rel="search" type="application/opensearchdescription+xml" href="https://raid.wiki.kernel.org/opensearch_desc.php" title="Linux Raid Wiki (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://raid.wiki.kernel.org/api.php?action=rsd">
<link rel="alternate" type="application/atom+xml" title="Linux Raid Wiki Atom feed" href="https://raid.wiki.kernel.org/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="stylesheet" href="./RAID setup - Linux Raid Wiki_files/load.php">
<!--[if IE 6]><link rel="stylesheet" href="/skins/monobook/IE60Fixes.css?303" media="screen" /><![endif]-->
<!--[if IE 7]><link rel="stylesheet" href="/skins/monobook/IE70Fixes.css?303" media="screen" /><![endif]--><style type="text/css" media="all">.js-messagebox{margin:1em 5%;padding:0.5em 2.5%;border:1px solid #ccc;background-color:#fcfcfc;font-size:0.8em}.js-messagebox .js-messagebox-group{margin:1px;padding:0.5em 2.5%;border-bottom:1px solid #ddd}.js-messagebox .js-messagebox-group:last-child{border-bottom:thin none transparent}

/* cache key: korg_mediawiki_raid:resourceloader:filter:minify-css:7:8b08bdc91c52a9ffba396dccfb5b473c */


.mw-collapsible-toggle{float:right} li .mw-collapsible-toggle{float:none} .mw-collapsible-toggle-li{list-style:none}

/* cache key: korg_mediawiki_raid:resourceloader:filter:minify-css:7:4250852ed2349a0d4d0fc6509a3e7d4c */
</style><meta name="ResourceLoaderDynamicStyles" content="">
<style>a:lang(ar),a:lang(ckb),a:lang(fa),a:lang(kk-arab),a:lang(mzn),a:lang(ps),a:lang(ur){text-decoration:none}a.new,#quickbar a.new{color:#ba0000}

/* cache key: korg_mediawiki_raid:resourceloader:filter:minify-css:7:c88e2bcd56513749bec09a7e29cb3ffa */
</style>

<script src="./RAID setup - Linux Raid Wiki_files/load(1).php"></script><script src="./RAID setup - Linux Raid Wiki_files/load(2).php"></script>
<script>if(window.mw){
mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"RAID_setup","wgTitle":"RAID setup","wgCurRevisionId":5321,"wgArticleId":1383,"wgIsArticle":true,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":[],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgRelevantPageName":"RAID_setup","wgRestrictionEdit":[],"wgRestrictionMove":[]});
}</script><script>if(window.mw){
mw.loader.implement("user.options",function($){mw.user.options.set({"ccmeonemails":0,"cols":80,"date":"default","diffonly":0,"disablemail":0,"disablesuggest":0,"editfont":"default","editondblclick":0,"editsection":1,"editsectiononrightclick":0,"enotifminoredits":0,"enotifrevealaddr":0,"enotifusertalkpages":1,"enotifwatchlistpages":0,"extendwatchlist":0,"externaldiff":0,"externaleditor":0,"fancysig":0,"forceeditsummary":0,"gender":"unknown","hideminor":0,"hidepatrolled":0,"highlightbroken":1,"imagesize":2,"justify":0,"math":1,"minordefault":0,"newpageshidepatrolled":0,"nocache":0,"noconvertlink":0,"norollbackdiff":0,"numberheadings":0,"previewonfirst":0,"previewontop":1,"quickbar":5,"rcdays":7,"rclimit":50,"rememberpassword":0,"rows":25,"searchlimit":20,"showhiddencats":0,"showjumplinks":1,"shownumberswatching":1,"showtoc":1,"showtoolbar":1,"skin":"monobook","stubthreshold":0,"thumbsize":2,"underline":2,"uselivepreview":0,"usenewrc":0,"watchcreations":0,"watchdefault":0,"watchdeletion":
0,"watchlistdays":3,"watchlisthideanons":0,"watchlisthidebots":0,"watchlisthideliu":0,"watchlisthideminor":0,"watchlisthideown":0,"watchlisthidepatrolled":0,"watchmoves":0,"wllimit":250,"variant":"en","language":"en","searchNs0":true,"searchNs1":false,"searchNs2":false,"searchNs3":false,"searchNs4":false,"searchNs5":false,"searchNs6":false,"searchNs7":false,"searchNs8":false,"searchNs9":false,"searchNs10":false,"searchNs11":false,"searchNs12":false,"searchNs13":false,"searchNs14":false,"searchNs15":false});;},{},{});mw.loader.implement("user.tokens",function($){mw.user.tokens.set({"editToken":"+\\","watchToken":false});;},{},{});

/* cache key: korg_mediawiki_raid:resourceloader:filter:minify-js:7:74a832f2292f1f4d40d425d223444e78 */
}</script>
<script>if(window.mw){
mw.loader.load(["mediawiki.page.startup","mediawiki.legacy.wikibits","mediawiki.legacy.ajax"]);
}</script><script type="text/javascript" src="./RAID setup - Linux Raid Wiki_files/load(3).php"></script>
</head>
<body class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-RAID_setup skin-monobook action-view">
<div id="globalWrapper">
<div id="column-content"><div id="content"><div id="mw-js-message" class="js-messagebox" style="display: none;"></div>
	<a id="top"></a>
	
	<h1 id="firstHeading" class="firstHeading"><span dir="auto">RAID setup</span></h1>
	<div id="bodyContent" class="mw-body">
		<div id="siteSub">From Linux Raid Wiki</div>
		<div id="contentSub"></div>
		<div id="jump-to-nav" class="mw-jump">Jump to: <a href="https://raid.wiki.kernel.org/index.php/RAID_setup#column-one">navigation</a>, <a href="https://raid.wiki.kernel.org/index.php/RAID_setup#searchInput">search</a></div>
		<!-- start content -->
<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><table style="border:1px solid #aaaaaa; background-color:#f9f9f9;width:100%; font-family: Verdana, sans-serif;">

<tbody><tr>
<td> Back to <a href="https://raid.wiki.kernel.org/index.php/Hardware_issues" title="Hardware issues">Hardware issues</a> <span style="float:right; padding-left:5px;">Forward to <a href="https://raid.wiki.kernel.org/index.php/Detecting,_querying_and_testing" title="Detecting, querying and testing">Detecting, querying and testing</a></span>
</td></tr></tbody></table>
<table id="toc" class="toc"><tbody><tr><td><div id="toctitle"><h2>Contents</h2><span class="toctoggle">&nbsp;[<a href="https://raid.wiki.kernel.org/index.php/RAID_setup#" class="internal" id="togglelink">hide</a>]&nbsp;</span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#RAID_setup"><span class="tocnumber">1</span> <span class="toctext">RAID setup</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#General_setup"><span class="tocnumber">1.1</span> <span class="toctext">General setup</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Preparing_and_partitioning_your_disk_devices"><span class="tocnumber">1.2</span> <span class="toctext">Preparing and partitioning your disk devices</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Downloading_and_installing_mdadm_-_the_RAID_management_tool"><span class="tocnumber">1.3</span> <span class="toctext">Downloading and installing mdadm - the RAID management tool</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Mdadm_modes_of_operation"><span class="tocnumber">1.4</span> <span class="toctext">Mdadm modes of operation</span></a>
<ul>
<li class="toclevel-3 tocsection-6"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#1._Create"><span class="tocnumber">1.4.1</span> <span class="toctext">1. Create</span></a></li>
<li class="toclevel-3 tocsection-7"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#2._Assemble"><span class="tocnumber">1.4.2</span> <span class="toctext">2. Assemble</span></a></li>
<li class="toclevel-3 tocsection-8"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#3._Follow_or_Monitor"><span class="tocnumber">1.4.3</span> <span class="toctext">3. Follow or Monitor</span></a></li>
<li class="toclevel-3 tocsection-9"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#4._Build"><span class="tocnumber">1.4.4</span> <span class="toctext">4. Build</span></a></li>
<li class="toclevel-3 tocsection-10"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#5._Grow"><span class="tocnumber">1.4.5</span> <span class="toctext">5. Grow</span></a></li>
<li class="toclevel-3 tocsection-11"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#6._Manage"><span class="tocnumber">1.4.6</span> <span class="toctext">6. Manage</span></a></li>
<li class="toclevel-3 tocsection-12"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#7._Misc"><span class="tocnumber">1.4.7</span> <span class="toctext">7. Misc</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-13"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Create_RAID_device"><span class="tocnumber">1.5</span> <span class="toctext">Create RAID device</span></a>
<ul>
<li class="toclevel-3 tocsection-14"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Linear_mode"><span class="tocnumber">1.5.1</span> <span class="toctext">Linear mode</span></a></li>
<li class="toclevel-3 tocsection-15"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#RAID-0"><span class="tocnumber">1.5.2</span> <span class="toctext">RAID-0</span></a></li>
<li class="toclevel-3 tocsection-16"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#RAID-1"><span class="tocnumber">1.5.3</span> <span class="toctext">RAID-1</span></a></li>
<li class="toclevel-3 tocsection-17"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#RAID-4.2F5.2F6"><span class="tocnumber">1.5.4</span> <span class="toctext">RAID-4/5/6</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-18"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Saving_your_RAID_configuration_.282011.29"><span class="tocnumber">1.6</span> <span class="toctext">Saving your RAID configuration (2011)</span></a></li>
<li class="toclevel-2 tocsection-19"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Create_and_mount_filesystem"><span class="tocnumber">1.7</span> <span class="toctext">Create and mount filesystem</span></a></li>
<li class="toclevel-2 tocsection-20"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Using_the_Array"><span class="tocnumber">1.8</span> <span class="toctext">Using the Array</span></a></li>
<li class="toclevel-2 tocsection-21"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#The_Persistent_Superblock_.282011.29"><span class="tocnumber">1.9</span> <span class="toctext">The Persistent Superblock (2011)</span></a></li>
<li class="toclevel-2 tocsection-22"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#External_Metadata_.282011.29"><span class="tocnumber">1.10</span> <span class="toctext">External Metadata (2011)</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-23"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Advanced_Options"><span class="tocnumber">2</span> <span class="toctext">Advanced Options</span></a>
<ul>
<li class="toclevel-2 tocsection-24"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Chunk_sizes"><span class="tocnumber">2.1</span> <span class="toctext">Chunk sizes</span></a>
<ul>
<li class="toclevel-3 tocsection-25"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#RAID-0_2"><span class="tocnumber">2.1.1</span> <span class="toctext">RAID-0</span></a></li>
<li class="toclevel-3 tocsection-26"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#RAID-0_with_ext2"><span class="tocnumber">2.1.2</span> <span class="toctext">RAID-0 with ext2</span></a></li>
<li class="toclevel-3 tocsection-27"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#RAID-1_2"><span class="tocnumber">2.1.3</span> <span class="toctext">RAID-1</span></a></li>
<li class="toclevel-3 tocsection-28"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#RAID-4"><span class="tocnumber">2.1.4</span> <span class="toctext">RAID-4</span></a></li>
<li class="toclevel-3 tocsection-29"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#RAID-5"><span class="tocnumber">2.1.5</span> <span class="toctext">RAID-5</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-30"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#ext2.2C_ext3.2C_and_ext4_.282011.29"><span class="tocnumber">2.2</span> <span class="toctext">ext2, ext3, and ext4 (2011)</span></a>
<ul>
<li class="toclevel-3 tocsection-31"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Calculation"><span class="tocnumber">2.2.1</span> <span class="toctext">Calculation</span></a></li>
<li class="toclevel-3 tocsection-32"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Performance"><span class="tocnumber">2.2.2</span> <span class="toctext">Performance</span></a></li>
<li class="toclevel-3 tocsection-33"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Changing_after_creation"><span class="tocnumber">2.2.3</span> <span class="toctext">Changing after creation</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-34"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup#XFS"><span class="tocnumber">2.3</span> <span class="toctext">XFS</span></a></li>
</ul>
</li>
</ul>
</td></tr></tbody></table>
<h1> <span class="mw-headline" id="RAID_setup">RAID setup</span></h1>
<h2> <span class="mw-headline" id="General_setup">General setup</span></h2>
<p>This is what you need for any of the RAID levels:
</p>
<ul><li>  A kernel with the appropriate <tt>md</tt> support either as modules or built-in. Preferably a kernel from the 4.x series. Although most of this should work fine with later 3.x kernels, too.
</li></ul>
<ul><li>  The mdadm tool
</li></ul>
<ul><li>  Patience, Pizza, and your favorite caffeinated beverage.
</li></ul>
<p>The first two items are included as standard in most GNU/Linux distributions
today.
</p><p>If your system has RAID support, you should have a file called
<a href="https://raid.wiki.kernel.org/index.php/Mdstat" title="Mdstat">/proc/mdstat</a>.  Remember it, that file is your friend. If you do not
have that file, maybe your kernel does not have RAID support. 
</p><p>If you're sure your kernel has RAID support you may need to run run modprobe raid[RAID mode] to load raid support into your kernel. 
eg to support raid5:
</p>
<pre>modprobe raid456
</pre>
<p>See what the file contains, by doing a
</p>
<pre>cat /proc/mdstat
</pre>
<p>It should tell you that you have the right RAID personality (eg. RAID mode) registered, and that
no RAID devices are currently active. See the <a href="https://raid.wiki.kernel.org/index.php/Mdstat" title="Mdstat">/proc/mdstat</a> page for more details.
</p>
<h2> <span class="mw-headline" id="Preparing_and_partitioning_your_disk_devices">Preparing and partitioning your disk devices</span></h2>
<p>Arrays can be built on top of entire disks or on partitions.
</p><p>This leads to 2 frequent questions:
</p>
<ul><li> Should I use entire device or a partition?
</li><li> What partition type?
</li></ul>
<p>Which are discussed in <a href="https://raid.wiki.kernel.org/index.php/Partition_Types" title="Partition Types">Partition Types</a>
</p>
<h2> <span class="mw-headline" id="Downloading_and_installing_mdadm_-_the_RAID_management_tool">Downloading and installing mdadm - the RAID management tool</span></h2>
<p>mdadm is now the standard RAID management tool and should be found in any modern distribution.
</p><p>You can retrieve the most recent version of mdadm with
</p>
<pre>git clone <a rel="nofollow" class="external free" href="git://neil.brown.name/mdadm">git://neil.brown.name/mdadm</a>
</pre>
<p>In the absence of any other preferences, do that in the /usr/local/src directory. As a linux-specific program there is none of this autoconf stuff - just follow the instructions as per the INSTALL file.
</p><p>Alternatively just use the normal distribution method for obtaining the package:
</p><p>Debian, Ubuntu:
</p>
<pre> apt-get install mdadm
</pre>
<p>Gentoo:
</p>
<pre> emerge mdadm
</pre>
<p>RedHat:
</p>
<pre> yum install mdadm
</pre>
<h2> <span class="mw-headline" id="Mdadm_modes_of_operation">Mdadm modes of operation</span></h2>
<p>mdadm is well documented in its manpage - well worth a read.
</p>
<pre>   man mdadm
</pre>
<p>mdadm has 7 major modes of operation. Normal operation just uses the 'Create', 'Assemble' and 'Monitor' commands - the rest come in handy when you're messing with your array; typically fixing it or changing it.
</p>
<h3> <span class="mw-headline" id="1._Create">1. Create</span></h3>
<p>Create a new array with per-device superblocks (normal creation).
</p>
<h3> <span class="mw-headline" id="2._Assemble">2. Assemble</span></h3>
<p>Assemble the parts of a previously created array into an active array.  Components  can  be  explicitly
given  or can be searched for.  mdadm checks that the components do form a bona fide array, and can, on
request, fiddle superblock information so as to assemble a faulty array. Typically you do this in the
init scripts after rebooting.
</p>
<h3> <span class="mw-headline" id="3._Follow_or_Monitor">3. Follow or Monitor</span></h3>
<p>Monitor  one or more md devices and act on any state changes.  This is only meaningful for raid1, 4, 5,
6, 10 or multipath arrays as only these have interesting state.  raid0 or linear  never  have  missing,
spare, or failed drives, so there is nothing to monitor. Typically you do this after rebooting too.
</p>
<h3> <span class="mw-headline" id="4._Build">4. Build</span></h3>
<p>Build an array that doesn't have per-device superblocks.  For these sorts of arrays, mdadm cannot
differentiate  between  initial  creation and subsequent assembly of an array.  It also cannot perform any
checks that appropriate devices have been requested.  Because of this, the Build mode  should  only  be
used together with a complete understanding of what you are doing.
</p>
<h3> <span class="mw-headline" id="5._Grow">5. Grow</span></h3>
<p><a href="https://raid.wiki.kernel.org/index.php/Growing" title="Growing">Grow</a>, shrink or otherwise reshape an  array in some way.  Currently supported growth options including changing the active size of component devices in RAID level 1/4/5/6 and changing the number of active devices in RAID1.
</p>
<h3> <span class="mw-headline" id="6._Manage">6. Manage</span></h3>
<p>This  is  for  doing  things  to specific components of an array such as adding new spares and removing
faulty devices.
</p>
<h3> <span class="mw-headline" id="7._Misc">7. Misc</span></h3>
<p>This is an 'everything else' mode that supports operations on active arrays,  operations  on  component devices such as erasing old superblocks, and information gathering operations.
</p><p><br>
</p>
<h2> <span class="mw-headline" id="Create_RAID_device">Create RAID device</span></h2>
<p>Below we'll see how to create arrays of various types; the basic approach is:
</p>
<pre>   mdadm --create /dev/md0 &lt;blah&gt;
   mdadm --monitor /dev/md0
</pre>
<p>If you want to access all the latest and upcoming features such as fully named RAID arrays so you no longer have to memorize which partition goes where, you'll want to make sure to use persistent metadata in the version 1.0 or higher format, as there is no way (currently or planned) to convert an array to a different metadata version. Current recommendations are to use metadata version 1.2 except when creating a boot partition, in which case use version 1.0 metadata and RAID-1.<a rel="nofollow" class="external autonumber" href="http://neil.brown.name/blog/20100519043730-002">[1]</a>
</p><p>Booting from a 1.2 raid is only supported when booting with an initramfs, as the kernel can no longer assemble or recognise an array - it relies on userspace tools. Booting directly from 1.0 is supported because the metadata is at the end of the array, and the start of a mirrored 1.0 array just looks like a normal partition to the kernel.
</p><p>NOTE: A work-around to upgrade metadata from version 0.90 to 1.0 is contained in the section <a href="https://raid.wiki.kernel.org/index.php/RAID_superblock_formats" title="RAID superblock formats">RAID superblock formats</a>.
</p><p>To change the metadata version (the default is now version 1.2 metadata) add the --metadata option <b>after</b> the switch stating what you're doing in the first place. This will work:
</p>
<pre>   mdadm --create /dev/md0 --metadata 1.0 &lt;blah&gt;
</pre>
<p>This, however, will not work:
</p>
<pre>   mdadm --metadata 1.0 --create /dev/md0 &lt;blah&gt;
</pre>
<h3> <span class="mw-headline" id="Linear_mode">Linear mode</span></h3>
<p>Ok, so you have two or more partitions which are not necessarily the
same size (but of course can be), which you want to append to each
other.
</p><p>Spare-disks are not supported here.  If a disk dies, the array dies
with it. There's no information to put on a spare disk.
</p><p>Using mdadm, a single command like
</p>
<pre>    mdadm --create --verbose /dev/md0 --level=linear --raid-devices=2 /dev/sdb6 /dev/sdc5
</pre>
<p>should create the array. The parameters talk for themselves.  The out-
put might look like this
</p>
<pre>   mdadm: chunk size defaults to 64K
   mdadm: array /dev/md0 started.
</pre>
<p>Have a look in <a href="https://raid.wiki.kernel.org/index.php/Mdstat" title="Mdstat">/proc/mdstat</a>. You should see that the array is running.
</p><p>Now, you can create a filesystem, just like you would on any other
device, mount it, include it in your /etc/fstab and so on.
</p>
<h3> <span class="mw-headline" id="RAID-0">RAID-0</span></h3>
<p>You have two or more devices, of approximately the same size, and you
want to combine their storage capacity and also combine their
performance by accessing them in parallel.
</p>
<pre>    mdadm --create --verbose /dev/md0 --level=stripe --raid-devices=2 /dev/sdb6 /dev/sdc5
</pre>
<p>Like in Linear mode, spare disks are not supported here either. RAID-0
has no redundancy, so when a disk dies, the array goes with it.
</p><p>Having run mdadm you have initialised the superblocks and
started the raid device.  Have a look in <a href="https://raid.wiki.kernel.org/index.php/Mdstat" title="Mdstat">/proc/mdstat</a> to see what's
going on. You should see that your device is now running.
</p><p>/dev/md0 is now ready to be formatted, mounted, used and abused.
</p>
<h3> <span class="mw-headline" id="RAID-1">RAID-1</span></h3>
<p>You have two devices of approximately same size, and you want the two
to be mirrors of each other. Eventually you have more devices, which
you want to keep as stand-by spare-disks, that will automatically
become a part of the mirror if one of the active devices break.
</p>
<pre>    mdadm --create --verbose /dev/md0 --level=mirror --raid-devices=2 /dev/sdb1 /dev/sdc1
</pre>
<p>If you have spare disks, you can add them to the end of the device
specification like
</p>
<pre>    mdadm --create --verbose /dev/md0 --level=mirror --raid-devices=2 /dev/sdb1 /dev/sdc1 --spare-devices=1 /dev/sdd1
</pre>
<p>Ok, now we're all set to start initializing the RAID. The mirror must
be constructed, eg. the contents (however unimportant now, since the
device is still not formatted) of the two devices must be
synchronized.
</p><p>Check out the <a href="https://raid.wiki.kernel.org/index.php/Mdstat" title="Mdstat">/proc/mdstat</a> file. It should tell you that the /dev/md0
device has been started, that the mirror is being reconstructed, and
an ETA of the completion of the reconstruction.
</p><p>Reconstruction is done using idle I/O bandwidth. So, your system
should still be fairly responsive, although your disk LEDs should be
glowing nicely.
</p><p>The reconstruction process is transparent, so you can actually use the
device even though the mirror is currently under reconstruction.
</p><p>Try formatting the device, while the reconstruction is running. It
will work.  Also you can mount it and use it while reconstruction is
running. Of Course, if the wrong disk breaks while the reconstruction
is running, you're out of luck.
</p>
<h3> <span class="mw-headline" id="RAID-4.2F5.2F6">RAID-4/5/6</span></h3>
<p>You have three or more devices (four or more for RAID-6) of roughly the same size, you want to
combine them into a larger device, but still to maintain a degree of
redundancy for data safety. Eventually you have a number of devices to
use as spare-disks, that will not take part in the array before
another device fails.
</p><p>If you use N devices where the smallest has size S, the size of the
entire raid-5 array will be (N-1)*S, or (N-2)*S for raid-6. This "missing" space is used for parity
(redundancy) information.  Thus, if any disk fails, all the data stays
intact. But if two disks fail on raid-5, or three on raid-6, all data is lost.
</p><p>The default chunk-size is 128kb.  That's the default io size on a spindle.
</p><p>Ok, enough talking. Let's see if raid-5 works. Run your command:
</p>
<pre>    mdadm --create --verbose /dev/md0 --level=5 --raid-devices=3 /dev/sdb1 /dev/sdc1 /dev/sdd1 --spare-devices=1 /dev/sde1
</pre>
<p>and see what happens.  Hopefully your disks start working
like mad, as they begin the reconstruction of your array. Have a look
in <a href="https://raid.wiki.kernel.org/index.php/Mdstat" title="Mdstat">/proc/mdstat</a> to see what's going on.
</p><p>If the device was successfully created, the reconstruction process has
now begun.  Your array is not consistent until this reconstruction
phase has completed. However, the array is fully functional (except
for the handling of device failures of course), and you can format it
and use it even while it is reconstructing.
</p><p>The initial reconstruction will always appear as though the array is degraded and is being reconstructed onto a spare, even if only just enough devices were added with zero spares. This is to optimize the initial reconstruction process. This may be confusing or worrying; it is intended for good reason. For more information, please check this <a rel="nofollow" class="external text" href="http://marc.info/?l=linux-raid&amp;m=112044009718483&amp;w=2">source, directly from Neil Brown</a>.
</p><p>Now, you can create a filesystem. See the section on special <a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Options_for_mke2fs">options to mke2fs</a> before formatting the filesystem. You can now mount it, include it in your /etc/fstab and so on.
</p>
<h2> <span class="mw-headline" id="Saving_your_RAID_configuration_.282011.29">Saving your RAID configuration (2011)</span></h2>
<p>After you've created your array, it's important to save the configuration in the proper mdadm configuration file. In Ubuntu, this is file /etc/mdadm/mdadm.conf. In some other distributions, this is file /etc/mdadm.conf. Check your distribution's documentation, or look at man mdadm.conf, to see what applies to your distribution.
</p><p>To save the configuration information:
</p><p>Ubuntu:
</p>
<pre>mdadm --detail --scan &gt;&gt; /etc/mdadm/mdadm.conf
</pre>
<p>Others (check your distribution's documentation):
</p>
<pre>mdadm --detail --scan &gt;&gt; /etc/mdadm.conf
</pre>
<p>Note carefully that if you do this before your array has finished initialization, you may have an inaccurate spares= clause. 
</p><p>In Ubuntu, if you neglect to save the RAID creation information, you will get peculiar errors when you try to assemble the RAID device (described below). There will be errors generated that the hard drive is busy, even though it seems to be unused. For example, the error might be similar to this: "mdadm: Cannot open /dev/sdd1: Device or resource busy". This happens because if there is no RAID configuration information in the mdadm.conf file, the system may create a RAID device from one disk in the array, activate it, and leave it unmounted. You can identify this problem by looking at the output of "cat /proc/mdstat". If it lists devices such as "md_d0" that are not part of your RAID setup, then first stop the extraneous device (for example: "mdadm --stop /dev/md_d0") and then try to assemble your RAID array as described below.
</p>
<h2> <span class="mw-headline" id="Create_and_mount_filesystem">Create and mount filesystem</span></h2>
<p>Have a look in <a href="https://raid.wiki.kernel.org/index.php/Mdstat" title="Mdstat">/proc/mdstat</a>. You should see that the array is running.
</p><p>Now, you can create a filesystem, just like you would on any other device, mount it, include it in your /etc/fstab, and so on.
</p><p>Common filesystem creation commands are mk2fs and mkfs.ext3. Please see <a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Options_for_mke2fs">options for mke2fs</a> for an example and details.
</p><p><br>
</p>
<h2> <span class="mw-headline" id="Using_the_Array">Using the Array</span></h2>
<p>Stopping a running RAID device is easy:
</p>
<pre>   mdadm --stop /dev/md0
</pre>
<p>Starting is a little more complex; you may think that:
</p>
<pre>   mdadm --run /dev/md0
</pre>
<p>would work - but it doesn't.
</p><p>Linux raid devices don't really exist on their own; they have to be assembled
each time you want to use them. Assembly is like creation insofar as it pulls
together devices
</p><p>If you earlier ran:
</p>
<pre>mdadm --create /dev/md0 --level=5 --raid-devices=4 /dev/sda1 /dev/sdb1 /dev/sdc1 /dev/sdd1
</pre>
<p>then
</p>
<pre>mdadm --assemble /dev/md0 /dev/sda1 /dev/sdb1 /dev/sdc1 /dev/sdd1
</pre>
<p>would work.
</p><p>However, the easy way to do this if you have a nice simple setup is:
</p>
<pre>  mdadm --assemble --scan 
</pre>
<p>For complex cases (ie you pull in disks from other machines that you're trying to repair) 
this has the potential to start arrays you don't really want started. A safer mechanism is to
use the uuid parameter and run:
</p>
<pre>  mdadm --scan --assemble --uuid=a26bf396:31389f83:0df1722d:f404fe4c
</pre>
<p>This will only assemble the array that you want - but it will work no matter
what has happened to the device names. This is particularly cool if, for example,
you add in a new SATA controller card and all of a sudden /dev/sda becomes /dev/sde!!!
</p>
<h2> <span class="mw-headline" id="The_Persistent_Superblock_.282011.29">The Persistent Superblock (2011)</span></h2>
<p>Back in "The Good Old Days" (TM), the raidtools would read your
/etc/raidtab file, and then initialize the array.  However, this would
require that the filesystem on which /etc/raidtab resided was mounted.
This was unfortunate if you want to boot on a RAID.
</p><p>Also, the old approach led to complications when mounting filesystems
on RAID devices. They could not be put in the /etc/fstab file as
usual, but would have to be mounted from the init-scripts.
</p><p>The persistent superblocks solve these problems. When an array is
created with the persistent-superblock option (the default now),
a special superblock is written to a location (different for 
different superblock versions) on all disks
participating in the array. This allows the kernel to read the
configuration of RAID devices directly from the disks involved,
instead of reading from some configuration file that may not be
available at all times.
</p><p>It's not a bad idea to maintain a consistent /etc/mdadm.conf file,
since you may need this file for later recovery of the array, although this is pretty much totally unnecessary today.
</p><p>A persistent superblock is mandatory for auto-assembly of
your RAID devices upon system boot.
</p><p>NOTE: Were persistent superblocks necessary for kernel raid support? This support has been moved into user space so this section may (or may not) be seriously out of date.
</p><p>Superblock physical layouts are listed on <a href="https://raid.wiki.kernel.org/index.php/RAID_superblock_formats" title="RAID superblock formats">RAID superblock formats</a> .
</p>
<h2> <span class="mw-headline" id="External_Metadata_.282011.29"> External Metadata (2011) </span></h2>
<p>MDRAID has always used its own metadata format. There are two different major formats for the MDRAID native metadata, the 0.90 and the version-1. The old 0.90 format limits the arrays to 28 components and 2 terabytes. With the latest mdadm, version 1.2 is the default.
</p><p>Starting with Linux kernel v2.6.27 and mdadm v3.0, external metadata are supported. These formats have been long supported with DMRAID and allow the booting of RAID volumes from Option ROM depending on the vendor.
</p><p>The first format is the DDF (Disk Data Format) defined by SNIA as the "Industry Standard" RAID metadata format. When a DDF array is constructed, a <a href="https://raid.wiki.kernel.org/index.php/Container" title="Container">container</a> is created in which normal RAID arrarys can be created within the container.
</p><p>The second format is the Intel(r) Matrix Storage Manager metadata format. This also creates
a <a href="https://raid.wiki.kernel.org/index.php/Container" title="Container">container</a> that is managed similar to DDF. And on some platforms (depending on vendor), this
format is supported by option-ROM in order to allow booting.
<a rel="nofollow" class="external autonumber" href="http://www.intel.com/design/chipsets/matrixstorage_sb.htm">[2]</a>
</p><p><br>
To report the RAID information from the Option ROM:
</p>
<pre>   mdadm --detail-platform
</pre>
<pre> Platform&nbsp;: Intel(R) Matrix Storage Manager
         Version&nbsp;: 8.9.0.1023
     RAID Levels&nbsp;: raid0 raid1 raid10 raid5
     Chunk Sizes&nbsp;: 4k 8k 16k 32k 64k 128k
       Max Disks&nbsp;: 6
     Max Volumes&nbsp;: 2
  I/O Controller&nbsp;: /sys/devices/pci0000:00/0000:00:1f.2
           Port0&nbsp;: /dev/sda (3MT0585Z)
           Port1&nbsp;: - non-disk device (ATAPI DVD D  DH16D4S) -
           Port2&nbsp;: /dev/sdb (WD-WCANK2850263)
           Port3&nbsp;: /dev/sdc (3MT005ML)
           Port4&nbsp;: /dev/sdd (WD-WCANK2850441)
           Port5&nbsp;: /dev/sde (WD-WCANK2852905)
           Port6&nbsp;: - no device attached –
</pre>
<p>To create RAID volumes that are external metadata, we must first create a container:
</p>
<pre>   mdadm --create --verbose /dev/md/imsm /dev/sd[b-g] --raid-devices 4 --metadata=imsm
</pre>
<p>In this example we created an IMSM based container for 4 RAID devices. Now we can create volumes within the container.
</p>
<pre>   mdadm --create --verbose /dev/md/vol0 /dev/md/imsm --raid-devices 4 --level 5
</pre>
<p>Of course, the --size option can be used to limit the size of the disk space used in the volume during creation in order to create multiple volumes within the container. One important note is that the various volumes within the container MUST span the same disks. i.e. a RAID10 volume and a RAID5 volume spanning the same number of disks.
</p>
<h1> <span class="mw-headline" id="Advanced_Options">Advanced Options</span></h1>
<h2> <span class="mw-headline" id="Chunk_sizes">Chunk sizes</span></h2>
<p>The chunk-size deserves an explanation.  You can never write
completely parallel to a set of disks. If you had two disks and wanted
to write a byte, you would have to write four bits on each disk. 
Actually, every second bit would go to disk 0 and the others to disk
1. Hardware just doesn't support that.  Instead, we choose some chunk-
size, which we define as the smallest "atomic" mass of data that can
be written to the devices.  A write of 16 kB with a chunk size of 4
kB will cause the first and the third 4 kB chunks to be written to
the first disk and the second and fourth chunks to be written to the
second disk, in the RAID-0 case with two disks.  Thus, for large
writes, you may see lower overhead by having fairly large chunks,
whereas arrays that are primarily holding small files may benefit more
from a smaller chunk size.
</p><p>Chunk sizes must be specified for all RAID levels, including linear
mode. However, the chunk-size does not make any difference for linear
mode.
</p><p>For optimal performance, you should experiment with the chunk-size, as well
as with the block-size of the filesystem you put on the array. For others experiments and performance charts, check out our <a href="https://raid.wiki.kernel.org/index.php/Performance" title="Performance">Performance</a> page. You can get chunk-size graphs galore.
</p>
<h4> <span class="mw-headline" id="RAID-0_2">RAID-0</span></h4>
<p>Data is written "almost" in parallel to the disks in the array.
Actually, chunk-size bytes are written to each disk, serially.
</p><p>If you specify a 4 kB chunk size, and write 16 kB to an array of three
disks, the RAID system will write 4 kB to disks 0, 1 and 2, in
parallel, then the remaining 4 kB to disk 0.
</p><p>A 32 kB chunk-size is a reasonable starting point for most arrays. But
the optimal value depends very much on the number of drives involved,
the content of the file system you put on it, and many other factors.
Experiment with it, to get the best performance.
</p><p><br>
</p>
<h4> <span class="mw-headline" id="RAID-0_with_ext2">RAID-0 with ext2</span></h4>
<p>The following tip was contributed by michael@freenet-ag.de:
</p><p>NOTE: this tip is no longer needed since the ext2 fs supports dedicated options: see "Options for mke2fs" below
</p><p>There is more disk activity at the beginning of ext2fs block groups.
On a single disk, that does not matter, but it can hurt RAID0, if all
block groups happen to begin on the same disk.
</p><p>Example:
</p><p>With a raid using a chunk size of 4k (also called stride-size), and filesystem using a block size of 4k, each block occupies one stride.
With two disks, the #disk * stride-size product (also called stripe-width) is 2*4k=8k.
The default block group size is 32768 blocks, which is a multiple of the stripe-width of 2 blocks, so all block groups start on disk 0,
which can easily become a hot spot, thus reducing overall performance.
Unfortunately, the block group size can only be set in steps of 8 blocks (32k when using 4k blocks), which also happens to be a multiple of the stripe-width,
so you can not avoid the problem by adjusting the blocks per group with the -g option of mkfs(8).
</p><p>If you add a disk, the stripe-width (#disk * stride-size product) is 12k,
so the first block group starts on disk 0, the second block group starts on disk 2 and the third on disk 1.
The load caused by disk activity at the block group beginnings spreads over all disks.
</p><p>In case you can not add a disk, try a stride size of 32k. The stripe-width (#disk * stride-size product) is then 64k.
Since you can change the block group size in steps of 8 blocks (32k), using 32760 blocks per group solves the problem.
</p><p>Additionally, the block group boundaries should fall on stride boundaries. The examples above get this right.
</p>
<h4> <span class="mw-headline" id="RAID-1_2">RAID-1</span></h4>
<p>For writes, the chunk-size doesn't affect the array, since all data
must be written to all disks no matter what.  For reads however, the
chunk-size specifies how much data to read serially from the
participating disks.  Since all active disks in the array contain the
same information, the RAID layer has complete freedom in choosing from
which disk information is read - this is used by the RAID code to
improve average seek times by picking the disk best suited for any
given read operation.
</p>
<h4> <span class="mw-headline" id="RAID-4">RAID-4</span></h4>
<p>When a write is done on a RAID-4 array, the parity information must be
updated on the parity disk as well.
</p><p>The chunk-size affects read performance in the same way as in RAID-0,
since reads from RAID-4 are done in the same way.
</p><p><br>
</p>
<h4> <span class="mw-headline" id="RAID-5">RAID-5</span></h4>
<p>On RAID-5, the chunk size has the same meaning for reads as for
RAID-0. Writing on RAID-5 is a little more complicated: When a chunk
is written on a RAID-5 array, the corresponding parity chunk must be
updated as well. Updating a parity chunk requires either
</p>
<ul><li> The original chunk, the new chunk, and the old parity block
</li></ul>
<ul><li> Or, all chunks (except for the parity chunk) in the stripe
</li></ul>
<p>The RAID code will pick the easiest way to update each parity chunk
as the write progresses. Naturally, if your server has lots of
memory and/or if the writes are nice and linear, updating the
parity chunks will only impose the overhead of one extra write
going over the bus (just like RAID-1). The parity calculation
itself is extremely efficient, so while it does of course load the
main CPU of the system, this impact is negligible. If the writes
are small and scattered all over the array, the RAID layer will
almost always need to read in all the untouched chunks from each
stripe that is written to, in order to calculate the parity chunk.
This will impose extra bus-overhead and latency due to extra reads.
</p><p>A reasonable chunk-size for RAID-5 is 128 kB. A study showed that with 4 drives (even-number-of-drives might make a difference) that large chunk sizes of 512-2048 kB gave superior results <a rel="nofollow" class="external autonumber" href="http://blog.jamponi.net/2008/07/raid56-and-10-benchmarks-on-26255_10.html">[3]</a>. As always, you may want to experiment with this or check out our <a href="https://raid.wiki.kernel.org/index.php/Performance" title="Performance">Performance</a> page.
</p><p>Also see the section on special <a href="https://raid.wiki.kernel.org/index.php/RAID_setup#Options_for_mke2fs">options to mke2fs</a>. This affects
RAID-5 performance.
</p><p><br>
</p>
<h2> <span class="mw-headline" id="ext2.2C_ext3.2C_and_ext4_.282011.29">ext2, ext3, and ext4 (2011)</span></h2>
<p>There are special options available when formatting RAID-4 or -5 devices with mke2fs or mkfs. The -E stride=nn,stripe-width=mm options will allow mke2fs to better place different ext2/ext3 specific data-structures in an intelligent way on the RAID device.
</p><p>Note: The commands mkfs or mkfs.ext3 or mkfs.ext2 are all versions of the same command, with the same options; use whichever is supported, and decide whether you are using ext2 or ext3 (non-journaled vs journaled). See the two versions of the same command below; each makes a different filesystem type.
</p><p>Note that ext3 no longer exists in the kernel - it has been subsumed into the ext4 driver, although ext3 filesystems can still be created and used.
</p><p>Here is an example, with its explanation below:
</p>
<pre>   mke2fs -v -m .1 -b 4096 -E stride=32,stripe-width=64 /dev/md0
   or
   mkfs.ext3 -v -m .1 -b 4096 -E stride=32,stripe-width=64 /dev/md0
</pre>
<pre>   Options explained:
     The first command makes a ext2 filesystem, the second makes a ext3 filesystem
     -v verbose
     -m .1 leave .1% of disk to root (so it doesnt fill and cause problems)
     -b 4096 block size of 4kb (recommended above for large-file systems)
     -E stride=32,stripe-width=64 see below calculation
</pre>
<h3> <span class="mw-headline" id="Calculation">Calculation</span></h3>
<ul><li> chunk size = 128kB (set by mdadm cmd, see chunk size advise above)
</li><li> block size = 4kB (recommended for large files, and most of time)
</li><li> stride = chunk / block = 128kB / 4k = 32
</li><li> stripe-width = stride * ( (n disks in raid5) - 1 ) = 32 * ( (3) - 1 ) = 32 * 2 = 64
</li></ul>
<p>If the chunk-size is 128 kB, it means, that 128 kB of consecutive data will reside on one disk. If we want to build an ext2 filesystem with 4 kB block-size, we realize that there will be 32 filesystem blocks in one array chunk.
</p><p>stripe-width=64 is calculated by multiplying the stride=32 value with the number of data disks in the array. 
</p><p>A raid5 with n disks has n-1 data disks, one being reserved for parity. (Note: the mke2fs man page incorrectly states n+1; this is a known bug in the man-page docs that is now fixed.) A raid10 (1+0) with n disks is actually a raid 0 of n/2 raid1 subarrays with 2 disks each.
</p>
<h3> <span class="mw-headline" id="Performance">Performance</span></h3>
<p>RAID-{4,5,10} performance is severely influenced by the stride and stripe-width options. It is uncertain how the stride option will affect other RAID levels. If anyone has information on this, please add to the knowledge.
</p><p>The ext2fs blocksize severely influences the performance of the filesystem. You should always use 4kB block size on any filesystem larger than a few hundred megabytes, unless you store a very large number of very small files on it.
</p>
<h3> <span class="mw-headline" id="Changing_after_creation">Changing after creation</span></h3>
<p>It is possible to change the parameters with 
</p>
<pre>   tune2fs -E stride=n,stripe-width=m /dev/mdx
</pre>
<h2> <span class="mw-headline" id="XFS"> XFS </span></h2>
<p>xfsprogs and the mkfs.xfs utility <b>automatically</b> select the best stripe size and stripe width for underlying devices that support it, such as Linux software RAID devices. Earlier versions of xfs used a built-in libdisk and the GET_ARRAY_INFO ioctl to gather the information; newer versions make use of enhanced geometry detection in libblkid.  When using libblkid, accurate geometry may also be obtained from hardware RAID devices which properly export this information.
</p><p>To create XFS filesystems optimized for RAID arrays manually, you'll need two parameters:
</p>
<ul><li> <b>chunk size</b>: same as used with mdadm
</li><li> <b>number of "data" disks</b>: number of disks that store data, not disks used for parity or spares. For example:
<ul><li> RAID 0 with 2 disks: 2 data disks (n)
</li><li> RAID 1 with 2 disks: 1 data disk (n/2)
</li><li> RAID 10 with 10 disks: 5 data disks (n/2)
</li><li> RAID 5 with 6 disks (no spares): 5 data disks (n-1)
</li><li> RAID 6 with 6 disks (no spares): 4 data disks (n-2)
</li></ul>
</li></ul>
<p>With these numbers in hand, you then want to use mkfs.xfs's su and sw parameters when creating your filesystem.
</p>
<ul><li> <b>su</b>: Stripe unit, which is the RAID chunk size, in bytes
</li><li> <b>sw</b>: Multiplier of the stripe unit, i.e. number of data disks
</li></ul>
<p>If you've a 4-disk RAID 5 and are using a chunk size of 64 KiB, the command to use is:
</p>
<pre>mkfs -t xfs -d su=64k -d sw=3 /dev/md0
</pre>
<p>Alternately, you may use the sunit/swidth mkfs options to specify stripe unit and width in 512-byte-block units. For the array above, it could also be specified as:
</p>
<pre>mkfs -t xfs -d sunit=128 -d swidth=384 /dev/md0
</pre>
<p>The result is exactly the same; however, the su/sw combination is often simpler to remember. Beware that sunit/swidth are inconsistently used throughout XFS' utilities (see xfs_info below).
</p><p>To check the parameters in use for an XFS filesystem, use xfs_info.
</p>
<pre>xfs_info /dev/md0
</pre>
<pre>meta-data=/dev/md0               isize=256    agcount=32, agsize=45785440 blks
         =                       sectsz=4096  attr=2
data     =                       bsize=4096   blocks=1465133952, imaxpct=5
         =                       sunit=16     swidth=48 blks
naming   =version 2              bsize=4096   ascii-ci=0
log      =internal               bsize=4096   blocks=521728, version=2
         =                       sectsz=4096  sunit=1 blks, lazy-count=0
realtime =none                   extsz=196608 blocks=0, rtextents=0
</pre>
<p>Here, rather than displaying 512-byte units as used in mkfs.xfs, sunit and swidth are shown as multiples of the filesystem block size (bsize), another file system tunable. This inconsistency is for legacy reasons, and is not well-documented.
</p><p>For the above example, sunit (sunit×bsize = su, 16×4096 = 64 KiB) and swidth (swidth×bsize = sw, 48×4096 = 192 KiB) are optimal and correctly reported.
</p><p>While the stripe unit and stripe width cannot be changed after an XFS file system has been created, they can be overridden at mount time with the sunit/swidth options, similar to ones used by mkfs.xfs.
</p><p>From Documentation/filesystems/xfs.txt in the kernel tree:
</p>
<pre> sunit=value and swidth=value
       Used to specify the stripe unit and width for a RAID device or
       a stripe volume.  "value" must be specified in 512-byte block
       units.
       If this option is not specified and the filesystem was made on
       a stripe volume or the stripe width or unit were specified for
       the RAID device at mkfs time, then the mount system call will
       restore the value from the superblock.  For filesystems that
       are made directly on RAID devices, these options can be used
       to override the information in the superblock if the underlying
       disk layout changes after the filesystem has been created.
       The "swidth" option is required if the "sunit" option has been
       specified, and must be a multiple of the "sunit" value.
</pre>
<p>Source: <a rel="nofollow" class="external text" href="http://says.samat.org/">Samat Says: Tuning XFS for RAID</a>
</p>
<table style="border:1px solid #aaaaaa; background-color:#f9f9f9;width:100%; font-family: Verdana, sans-serif;">

<tbody><tr>
<td> Back to <a href="https://raid.wiki.kernel.org/index.php/Hardware_issues" title="Hardware issues">Hardware issues</a> <span style="float:right; padding-left:5px;">Forward to <a href="https://raid.wiki.kernel.org/index.php/Detecting,_querying_and_testing" title="Detecting, querying and testing">Detecting, querying and testing</a></span>
</td></tr></tbody></table>

<!-- 
NewPP limit report
Preprocessor node count: 137/1000000
Post‐expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Expensive parser function count: 0/100
-->

<!-- Saved in parser cache with key korg_mediawiki_raid:pcache:idhash:1383-0!*!0!!en!*!* and timestamp 20181213044127 -->
</div><div class="printfooter">
Retrieved from "<a href="https://raid.wiki.kernel.org/index.php?title=RAID_setup&amp;oldid=5321">https://raid.wiki.kernel.org/index.php?title=RAID_setup&amp;oldid=5321</a>"</div>
		<div id="catlinks" class="catlinks catlinks-allhidden"></div>		<!-- end content -->
				<div class="visualClear"></div>
	</div>
</div></div>
<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
				<li id="ca-nstab-main" class="selected"><a href="https://raid.wiki.kernel.org/index.php/RAID_setup" primary="1" context="subject" title="View the content page [alt-shift-c]" accesskey="c">Page</a></li>
				<li id="ca-talk" class="new"><a href="https://raid.wiki.kernel.org/index.php?title=Talk:RAID_setup&amp;action=edit&amp;redlink=1" primary="1" context="talk" title="Discussion about the content page [alt-shift-t]" accesskey="t">Discussion</a></li>
				<li id="ca-viewsource"><a href="https://raid.wiki.kernel.org/index.php?title=RAID_setup&amp;action=edit" primary="1" title="This page is protected.
You can view its source [alt-shift-e]" accesskey="e">View source</a></li>
				<li id="ca-history"><a href="https://raid.wiki.kernel.org/index.php?title=RAID_setup&amp;action=history" rel="archives" title="Past revisions of this page [alt-shift-h]" accesskey="h">History</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="https://raid.wiki.kernel.org/index.php?title=Special:UserLogin&amp;returnto=RAID+setup" title="You are encouraged to log in; however, it is not mandatory [alt-shift-o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
<a href="https://raid.wiki.kernel.org/index.php/Linux_Raid" style="background-image: url(/skins/common/images-raid/wiki.png);" title="Visit the main page"></a>
	</div>
	<div class="generated-sidebar portlet" id="p-navigation">
		<h5>Navigation</h5>
		<div class="pBody">
			<ul>
				<li id="n-mainpage-description"><a href="https://raid.wiki.kernel.org/index.php/Linux_Raid" title="Visit the main page [alt-shift-z]" accesskey="z">Main page</a></li>
				<li id="n-recentchanges"><a href="https://raid.wiki.kernel.org/index.php/Special:RecentChanges" title="A list of recent changes in the wiki [alt-shift-r]" accesskey="r">Recent changes</a></li>
				<li id="n-randompage"><a href="https://raid.wiki.kernel.org/index.php/Special:Random" title="Load a random page [alt-shift-x]" accesskey="x">Random page</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="https://raid.wiki.kernel.org/index.php" id="searchform">
				<input type="hidden" name="title" value="Special:Search">
				<input type="search" name="search" title="Search Linux Raid Wiki [alt-shift-f]" accesskey="f" id="searchInput">
				<input type="submit" name="go" value="Go" title="Go to a page with this exact name if exists" id="searchGoButton" class="searchButton">&nbsp;
				<input type="submit" name="fulltext" value="Search" title="Search the pages for this text" id="mw-searchButton" class="searchButton">
			</form>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Tools</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="https://raid.wiki.kernel.org/index.php/Special:WhatLinksHere/RAID_setup" title="A list of all wiki pages that link here [alt-shift-j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="https://raid.wiki.kernel.org/index.php/Special:RecentChangesLinked/RAID_setup" title="Recent changes in pages linked from this page [alt-shift-k]" accesskey="k">Related changes</a></li>
				<li id="t-specialpages"><a href="https://raid.wiki.kernel.org/index.php/Special:SpecialPages" title="A list of all special pages [alt-shift-q]" accesskey="q">Special pages</a></li>
				<li><a href="https://raid.wiki.kernel.org/index.php?title=RAID_setup&amp;printable=yes" rel="alternate">Printable version</a></li>
				<li id="t-permalink"><a href="https://raid.wiki.kernel.org/index.php?title=RAID_setup&amp;oldid=5321" title="Permanent link to this revision of the page">Permanent link</a></li>
			</ul>
		</div>
	</div>
</div><!-- end of the left (by default at least) column -->
<div class="visualClear"></div>
<div id="footer">
	<div id="f-poweredbyico">
		<a href="https://www.mediawiki.org/"><img src="./RAID setup - Linux Raid Wiki_files/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" width="88" height="31"></a>
	</div>
	<ul id="f-list">
		<li id="lastmod"> This page was last modified on 20 September 2016, at 13:51.</li>
		<li id="privacy"><a href="https://raid.wiki.kernel.org/index.php/Linux_Raid_Wiki:Privacy_policy" title="Linux Raid Wiki:Privacy policy">Privacy policy</a></li>
		<li id="about"><a href="https://raid.wiki.kernel.org/index.php/Linux_Raid_Wiki:About" title="Linux Raid Wiki:About">About Linux Raid Wiki</a></li>
		<li id="disclaimer"><a href="https://raid.wiki.kernel.org/index.php/Linux_Raid_Wiki:General_disclaimer" title="Linux Raid Wiki:General disclaimer">Disclaimers</a></li>
	</ul>
</div>
</div>
<script>if(window.mw){
mw.loader.load(["mediawiki.user","mediawiki.page.ready"], null, true);
}</script><script src="./RAID setup - Linux Raid Wiki_files/load(4).php" type="text/javascript"></script>
<!-- Served in 0.097 secs. --></body></html>